{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyPD0qr8eztnFQp/zO13E1/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anko191/Python_Kaggle/blob/master/Tensorflow/Try_Optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwNDj6hk4w1B"
      },
      "source": [
        "    #optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period = 10, \n",
        "    #                                    name = 'Lookahead_10')\n",
        "    # optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "    #optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4, nesterov=True)\n",
        "    # optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "    # optimizer = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "    # optimizer = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=1e-4)\n",
        "    optimizer = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=1e-4, amsgrad=False)\n",
        "    # optimizer = tfa.optimizers.Lookahead(keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=1e-4, amsgrad=False),                                         sync_period = 10, name = 'Lookahead')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc9b7ve-4x-U"
      },
      "source": [
        "## 実行\n",
        "* Importance_col only\n",
        "    * [ - 全部標準化しちゃった -]\n",
        "        - glorot_uniform\n",
        "            - 4096 loss 1.000 so slow 80s\n",
        "            * 2048 loss ?\n",
        "            * 2-layer ReLU 1024 loss first 0.3874 fast 7s -> 0.0484 \n",
        "            * 3-layer ReLU 1024 loss first 0.2645 fast -> 0.0499\n",
        "            * 2-layer ReLU 512 loss first 0.4885 -> 0.0449\n",
        "                * Dropout(0.2)\n",
        "            * 3-layer ReLU 512 loss first 0.3811 -> ?\n",
        "    * [ - startswith('cp-')のだけ -]\n",
        "        - glorot_uniform\n",
        "            * 3-layer ReLU 512 loss first 0.5000 -> 0.0473\n",
        "                * Dropout(0.2)\n",
        "            * 2-layer ReLU 512 loss first 0.5762 -> 0.0484\n",
        "                * Dropout(0.2)\n",
        "            * 2-layer ReLU 2048 loss first 0.5  -> 0.0469\n",
        "                * Dropout(0.5)\n",
        "        - not designate initializer\n",
        "            * 2-layer ReLU 2048 loss first 0.58 -> 0.0472\n",
        "     * 10/8 all normalization\n",
        "         * optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=1e-4, amsgrad=False)\n",
        "            * ReLU 1024 layer + sigmoid 206 layer\n",
        "                * 0.0695 -> 0.0165 ...\n",
        "         * tfa.forward below model ↑\n",
        "            * same\n",
        "                * 0.30 -> 0.0165\n",
        "* all_columns\n",
        "    * [ - startswith('cp-')のだけ -]\n",
        "        - not designate initializer\n",
        "            * 2-layer ReLU 2048 loss first 0.5988 -> 0.0472\n",
        "            * 2-layer ReLU 1024 loss first 0.4805 -> 0.0494\n",
        "        * Adam\n",
        "            * 2-layer 1024 relu 0.4 -> 0.04\n",
        "        * sgd keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4, nesterov=True)\n",
        "            * 2l 1024 relu 0.13 -> 0.13 ... orz\n",
        "        * keras.optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
        "            * 0.0541 !? -> 0.0525\n",
        "        * keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "            * 0.4108 -> 0.0471\n",
        "        * keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "            * 0.4173 -> 0.0474\n",
        "        * keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=1e-4)\n",
        "            * 0.2879 -> 0.0462\n",
        "    * 全部をNormalization\n",
        "        * Optimizer : Adadelta lr = 1.0, rho = 0.95, decay 1e-4 epsilon = None\n",
        "            * 206 のレイヤーのみ。てかsigmoidどっかで入れなあかんのでは？\n",
        "                * 0.28 -> 0.0443\n",
        "            * sigmoidにするか？\n",
        "                * 0.43 -> 0.0325 !? \n",
        "            * ReLU 1024 layer + sigmoid 206 layer\n",
        "                * 0.43 -> 0.0181 !??!?!?! yabee\n",
        "        * Optimzier : Adam \n",
        "        * optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=1e-4, amsgrad=False)\n",
        "            * ReLU 1024 layer + sigmoid 206 layer\n",
        "                * 0.44 -> 0.0127 !? leakage..?\n",
        "                * 0.44 -> 0.0164 -> 0.0160\n",
        "         * tfa.forward below model ↑\n",
        "            * same below\n",
        "                * 0.3124 -> 0.0162]\n",
        "         * not using...\n",
        "            * sigmoid 1024 -> ReLU 1024 -> sigmoid\n",
        "                * 0.060 -> 0.0162\n",
        "            * ReLU 1024 -> ReLU 1024 -> sigmoid\n",
        "                * 0.06 -> 0.0163\n",
        "            * sigmoid 206 only...\n",
        "                * 0.6 -> 0.05 No\n",
        "            * ReLU 206 only\n",
        "                * 0.6 -> 0.0476\n",
        "       * optimizer = keras.optimizers.Adam(★lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=1e-4, amsgrad=False)\n",
        "         * not using...\n",
        "            * sigmoid 1024 -> ReLU 1024 -> sigmoid\n",
        "                * "
      ]
    }
  ]
}